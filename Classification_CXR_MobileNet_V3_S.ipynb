{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUuIZMInzdxc",
        "outputId": "5bbfa932-0c12-4406-c853-3cad6153a03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  3 07:55:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqjgUukyQNvg"
      },
      "source": [
        "#Input dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niL2MGxWSqVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset from :  https://www.kaggle.com/datasets/francismon/curated-covid19-chest-xray-dataset"
      ],
      "metadata": {
        "id": "SfS_yDBHSrA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2AaagSjIzKA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNaDhQy_S2U0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile ('/content/drive/MyDrive/Dataset Noor Muhamad Rizki/X-ray dataset.zip')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yvEJZCF7t2XJ",
        "outputId": "0cdf5690-75ae-4c57-c7a1-a4f821caee90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5AUePr3SdnB"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history):\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  img = tf.io.decode_image(img)\n",
        "\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img"
      ],
      "metadata": {
        "id": "GPY8QExxRRaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1nFNnuI8lLA"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
        " \n",
        "\n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "  \n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "  \n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n",
        "  plt.xticks(rotation=70, fontsize=text_size)\n",
        "  plt.yticks(fontsize=text_size)\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjR7irR4cYyC"
      },
      "outputs": [],
      "source": [
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "\n",
        "    \n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8pcX5dn0sD6"
      },
      "outputs": [],
      "source": [
        "def tensorboard_callback(dir_name, experiment_name):\n",
        "\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlEmL9DWt8nS",
        "outputId": "bd7ad0b7-a737-49f3-ba39-a4dee2a6983d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tersedia 0 direktori dan  2 citra pada  dataset\n",
            "Tersedia 0 direktori dan  3 citra pada  dataset/train\n",
            "Tersedia 2616 direktori dan  0 citra pada  dataset/train/0_Normal\n",
            "Tersedia 3726 direktori dan  0 citra pada  dataset/train/2_Pneumonia\n",
            "Tersedia 1025 direktori dan  0 citra pada  dataset/train/1_Covid19\n",
            "Tersedia 0 direktori dan  3 citra pada  dataset/validation\n",
            "Tersedia 654 direktori dan  0 citra pada  dataset/validation/0_Normal\n",
            "Tersedia 931 direktori dan  0 citra pada  dataset/validation/2_Pneumonia\n",
            "Tersedia 256 direktori dan  0 citra pada  dataset/validation/1_Covid19\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "for dirpath, filenames, dirnames in os.walk('dataset'):\n",
        "  print (f\"Tersedia {len(dirnames)} direktori dan  {len(filenames)} citra pada  {dirpath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNI5ppPqQbSW"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU-NYVQhRKfL"
      },
      "outputs": [],
      "source": [
        "data_direktori = os.path.join ('/content/dataset')\n",
        "train_dir = os.path.join ('/content/dataset/train')\n",
        "validation_dir = os.path.join ('/content/dataset/validation')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhIQ0Qnfg4zB"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (224, 224) \n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "validation_data = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\",\n",
        "                                                               shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBdL9i2NVdyi"
      },
      "outputs": [],
      "source": [
        "class_names = train_data.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLgBGx4e0wX5"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('val_accuracy')>0.9814):\n",
        "      print(\"\\nReached 9815% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "maks = myCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0FAHmam1IW8"
      },
      "source": [
        "#Make Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ezPRsPPEeLr"
      },
      "outputs": [],
      "source": [
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.MobileNetV3Small(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(3, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "\n",
        "model= keras.Model(base_model.input, outputs)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history= model.fit(train_dir,\n",
        "                    epochs=50,\n",
        "                    validation_data=validation_dir,\n",
        "                    callbacks=[maks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmCPT-n4_FFn"
      },
      "outputs": [],
      "source": [
        "train_result = model.evaluate(train_dir)\n",
        "test_result = model.evaluate(validation_dir)\n",
        "\n",
        "df = pd.DataFrame(zip(train_result,test_result),columns=['Train','Val'],index=['Loss','Acc'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgqVOw0hDIdd"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxjIe4XdvoJi"
      },
      "source": [
        "#Melakukan Fine Tunning pada model tanpa Augmentasi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Dc8CH6bjV1"
      },
      "outputs": [],
      "source": [
        "# Unfreeze all of the layers in the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Refreeze every layer except for the last 5\n",
        "for layer in base_model.layers[:-23]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LHnRtwwbaDJ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4), # 10x lower learning rate than default\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 100 # model has already done 10 epochs, this is the total number of epochs we're after (5+5=10)\n",
        "\n",
        "history_fine = model.fit(train_data,\n",
        "                                                    epochs=fine_tune_epochs,\n",
        "                                                     validation_data=validation_data,\n",
        "                                                     initial_epoch=history.epoch[-1], # start from previous last epoch\n",
        "                                                     callbacks=[maks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2td65BMVEk8R"
      },
      "outputs": [],
      "source": [
        "train_result = model.evaluate(train_data)\n",
        "test_result = model.evaluate(validation_data)\n",
        "\n",
        "fine = pd.DataFrame(zip(train_result,test_result),columns=['Train','Val'],index=['Loss','Acc'])\n",
        "fine_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQPy93G9gCer"
      },
      "outputs": [],
      "source": [
        "compare_historys(original_history=history,\n",
        "                 new_history=history_fine,\n",
        "                 initial_epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation model"
      ],
      "metadata": {
        "id": "CG5UrwLyQsUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confussion matrix"
      ],
      "metadata": {
        "id": "bDnRZnYkQ5gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0yAFLIirUx7"
      },
      "outputs": [],
      "source": [
        "pred_probs= model.predict(validation_data, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5TimGBOriYy"
      },
      "outputs": [],
      "source": [
        "pred_classes = pred_probs.argmax(axis=1)\n",
        "y_labels = []\n",
        "for images, labels in validation_data.unbatch(): # unbatch the test data and get images and labels\n",
        "  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmHfPaqRru1C"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "sklearn_accuracy = accuracy_score(y_labels, pred_classes)\n",
        "sklearn_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YH3iy1_rz0L"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(y_true=y_labels,\n",
        "                      y_pred=pred_classes,\n",
        "                      classes=class_names,\n",
        "                      figsize=(10, 10),\n",
        "                      text_size=20,\n",
        "                      norm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f8Co8zusVhz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_labels, pred_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Most Wrong images"
      ],
      "metadata": {
        "id": "Bcqs4PEpQ-V_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8-CxeMPse5T"
      },
      "outputs": [],
      "source": [
        "filepaths = []\n",
        "for filepath in data_uji.list_files(\"/content/dataset/validation/*/*.jpg\", \n",
        "                                     shuffle=False):\n",
        "  filepaths.append(filepath.numpy())\n",
        "filepaths[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGiO0RIsGyZg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pred_df = pd.DataFrame({\"img_path\": filepaths,\n",
        "                        \"y_true\": y_labels,\n",
        "                        \"y_pred\": pred_classes,\n",
        "                        \"pred_conf\": pred_probs.max(axis=1), # get the maximum prediction probability value\n",
        "                        \"y_true_classname\": [class_names[i] for i in y_labels],\n",
        "                        \"y_pred_classname\": [class_names[i] for i in pred_classes]}) \n",
        "pred_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "rSDiS3nHRF8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False)[:100]\n",
        "top_100_wrong.head(50)"
      ],
      "metadata": {
        "id": "MTCMjc4ERLg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "\n",
        "  # Choose a random image from a random class \n",
        "filepath = '/content/dataset/validation/2_Pneumonia/Pneumonia-Viral (1363).jpg'\n",
        "class_name = class_names[2]\n",
        "\n",
        "  # Load the image and make predictions\n",
        "img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions\n",
        "pred_prob = best_model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n",
        "pred_class = class_names[pred_prob.argmax()] # find the predicted class \n",
        "\n",
        "  # Plot the image(s)\n",
        "plt.imshow(img/255.)\n",
        "if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n",
        "  title_color = \"g\"\n",
        "else:\n",
        "  title_color = \"r\"\n",
        "\n",
        "plt.title(f\"citra asli : {class_name} \\n prediksi: {pred_class}: {100*pred_prob.max():.2f}%\", c=title_color, fontsize=12,fontweight=20)\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "x01Y_4glRSnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-Cam Visualization"
      ],
      "metadata": {
        "id": "UFw6ADkZRain"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "IMAGE_PATH = filepath\n",
        "LAYER_NAME = 'Conv_1'\n",
        "CLASS_INDEX = 2\n",
        "\n",
        "img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\n",
        "img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# Load initial model\n",
        "model_grad = model_2\n",
        "\n",
        "# Create a graph that outputs target convolution and output\n",
        "grad_model = tf.keras.models.Model([model_grad.inputs], [model_grad.get_layer(LAYER_NAME).output, model_grad.output])\n",
        "\n",
        "# Get the score for target class\n",
        "with tf.GradientTape() as tape:\n",
        "    conv_outputs, predictions = grad_model(np.array([img]))\n",
        "    loss = predictions[:, CLASS_INDEX]\n",
        "\n",
        "# Extract filters and gradients\n",
        "output = conv_outputs[0]\n",
        "grads = tape.gradient(loss, conv_outputs)[0]\n",
        "\n",
        "# Average gradients spatially\n",
        "weights = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "# Build a ponderated map of filters according to gradients importance\n",
        "cam = np.ones(output.shape[0:2], dtype=np.float32)\n",
        "\n",
        "for index, w in enumerate(weights):\n",
        "    cam += w * output[:, :, index]\n",
        "\n",
        "# Heatmap visualization\n",
        "cam = cv2.resize(cam.numpy(), (224, 224))\n",
        "cam = np.maximum(cam, 0)\n",
        "heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n",
        "\n",
        "cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "\n",
        "output_image = cv2.addWeighted(cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2BGR), 0.5, cam, 1, 0)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(output_image)\n",
        "plt.title(f\"real image : {class_name} \\n prediction: {pred_class}: {100*pred_prob.max():.2f}%\", c=title_color, fontsize=18,fontweight=20)\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "GSqoL-VdRg8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}